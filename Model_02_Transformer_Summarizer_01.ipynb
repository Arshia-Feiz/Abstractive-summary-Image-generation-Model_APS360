{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2: Transformer-Based Summarization (Baseline)\n",
        "\n",
        "## Overview\n",
        "In this phase, we build a baseline abstractive summarization model using the Transformer architecture. This model will serve as a benchmark for comparison against more advanced models in subsequent phases.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Tokenization and Preprocessing\n",
        "- Load the cleaned dataset.\n",
        "- Construct a vocabulary based on the most frequent words.\n",
        "- Encode the input articles and summaries as sequences of integer indices.\n",
        "- Pad sequences to fixed length for batch processing.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Transformer Model Architecture\n",
        "- Use PyTorch's `nn.Transformer` or a custom encoder-decoder Transformer model.\n",
        "- Components:\n",
        "  - Token embedding layer\n",
        "  - Positional encoding\n",
        "  - Transformer Encoder\n",
        "  - Transformer Decoder\n",
        "  - Final linear layer to project decoder outputs to vocabulary size\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Masking Strategy\n",
        "- **Padding Mask:** Prevents the model from attending to padded elements.\n",
        "- **Look-ahead Mask:** Ensures that the decoder attends only to previous tokens during training.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Model Training\n",
        "- Loss function: `nn.CrossEntropyLoss(ignore_index=PAD)`\n",
        "- Optimizer: `Adam`\n",
        "- Training loop for multiple epochs\n",
        "- Log training and validation loss\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Evaluation\n",
        "- Implement greedy decoding during inference.\n",
        "- Compare model-generated summaries against ground-truth summaries.\n",
        "- Print sample predictions to assess performance.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Next Steps\n",
        "- Analyze failure cases.\n",
        "- Consider training with subword tokenization or pre-trained embeddings.\n",
        "- Prepare to fine-tune a pre-trained transformer in Phase 3.\n",
        "\n"
      ],
      "metadata": {
        "id": "VuGgSJmA1o3-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "ztZhtdJ51dWF",
        "outputId": "2256b796-3024-42aa-8c3e-197919ff69b8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5bcea576-ea7f-47a3-bffa-001285469cc9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5bcea576-ea7f-47a3-bffa-001285469cc9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cleaned_dataset.csv to cleaned_dataset.csv\n",
            "User uploaded file \"cleaned_dataset.csv\" with length 113592985 bytes\n",
            "                                                 URL  \\\n",
            "0  https://www.moneycontrol.com/news/business/eco...   \n",
            "1  https://www.businesstoday.in/top-story/state-r...   \n",
            "2  https://www.financialexpress.com/economy/covid...   \n",
            "3  https://www.moneycontrol.com/news/business/mar...   \n",
            "4  https://www.financialexpress.com/industry/six-...   \n",
            "\n",
            "                                             Content  \\\n",
            "0  us consumer spending dropped by a record in ap...   \n",
            "1  state-run lenders require an urgent rs 1.2 tri...   \n",
            "2  apparel exporters on wednesday urged the gover...   \n",
            "3  asian shares battled to extend a global reboun...   \n",
            "4  after india's sovereign credit rating fell to ...   \n",
            "\n",
            "                                             Summary Sentiment  \n",
            "0  consumer spending plunges 13.6 percent in apri...  Negative  \n",
            "1  government will have to take a bulk of the tab...  Negative  \n",
            "2  exporters are facing issues in terms of raw ma...  Negative  \n",
            "3  the dollar loses some ground on the safe haven...  Negative  \n",
            "4  six indian public-sector undertakings have tak...  Negative  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  df = pd.read_csv(fn)\n",
        "  print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')\n",
        "  break # Exit the loop after processing the first file\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"cleaned_dataset.csv\")\n",
        "\n",
        "# Rename columns for consistency\n",
        "df.rename(columns={'Content': 'text', 'Summary': 'summary', 'Sentiment': 'sentiment'}, inplace=True)\n",
        "\n",
        "# Drop rows with missing values\n",
        "df.dropna(subset=['text', 'summary', 'sentiment'], inplace=True)\n",
        "\n",
        "# Tokenization using TreebankWordTokenizer (no punkt_tab needed)\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "df['text_tokens'] = df['text'].apply(tokenizer.tokenize)\n",
        "df['summary_tokens'] = df['summary'].apply(lambda x: ['<SOS>'] + tokenizer.tokenize(x) + ['<EOS>'])\n",
        "\n",
        "# Encode sentiment labels (e.g., Positive = 2, Negative = 0, Neutral = 1)\n",
        "label_encoder = LabelEncoder()\n",
        "df['sentiment_encoded'] = label_encoder.fit_transform(df['sentiment'])\n",
        "\n",
        "# Split into train/val/test\n",
        "train_df, temp_df = train_test_split(df, test_size=0.15, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.33, random_state=42)  # 10% test, 5% val\n",
        "\n",
        "# Print class distribution for sanity check\n",
        "print(\"✅ Sentiment Labels:\", label_encoder.classes_)\n",
        "print(\"✅ Train size:\", len(train_df), \"Validation size:\", len(val_df), \"Test size:\", len(test_df))\n",
        "print(\"✅ Sample tokenized text:\", df['text_tokens'].iloc[0][:15])\n",
        "print(\"✅ Sample sentiment encoding:\", df['sentiment_encoded'].iloc[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5DnkMFx-4ywu",
        "outputId": "d88a0baf-53ba-487b-ec26-cc4e996295c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sentiment Labels: ['Negative' 'Neutral' 'Positive']\n",
            "✅ Train size: 22298 Validation size: 2636 Test size: 1299\n",
            "✅ Sample tokenized text: ['us', 'consumer', 'spending', 'dropped', 'by', 'a', 'record', 'in', 'april', 'as', 'the', 'covid-19', 'pandemic', 'undercut', 'demand']\n",
            "✅ Sample sentiment encoding: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import torch\n",
        "\n",
        "# Build vocab from train set only (text + summary)\n",
        "def build_vocab(token_lists, min_freq=2):\n",
        "    counter = Counter()\n",
        "    for tokens in token_lists:\n",
        "        counter.update(tokens)\n",
        "\n",
        "    vocab = {\n",
        "        '<PAD>': 0,\n",
        "        '<UNK>': 1,\n",
        "        '<SOS>': 2,\n",
        "        '<EOS>': 3\n",
        "    }\n",
        "    idx = 4\n",
        "    for word, freq in counter.items():\n",
        "        if freq >= min_freq and word not in vocab:\n",
        "            vocab[word] = idx\n",
        "            idx += 1\n",
        "    return vocab\n",
        "\n",
        "# Combine text and summary tokens for vocab\n",
        "combined_tokens = train_df['text_tokens'].tolist() + train_df['summary_tokens'].tolist()\n",
        "word2idx = build_vocab(combined_tokens)\n",
        "idx2word = {v: k for k, v in word2idx.items()}\n",
        "\n",
        "# Helper to convert tokens to indices\n",
        "def encode(tokens, word2idx):\n",
        "    return [word2idx.get(token, word2idx['<UNK>']) for token in tokens]\n",
        "\n",
        "# Apply encoding\n",
        "for df_ in [train_df, val_df, test_df]:\n",
        "    df_['text_idx'] = df_['text_tokens'].apply(lambda tokens: encode(tokens, word2idx))\n",
        "    df_['summary_idx'] = df_['summary_tokens'].apply(lambda tokens: encode(tokens, word2idx))\n",
        "\n",
        "# Define special token indices\n",
        "PAD = word2idx['<PAD>']\n",
        "UNK = word2idx['<UNK>']\n",
        "SOS = word2idx['<SOS>']\n",
        "EOS = word2idx['<EOS>']\n",
        "\n",
        "print(f\"✅ Vocab size: {len(word2idx)}\")\n",
        "print(f\"✅ Sample encoded input: {train_df['text_idx'].iloc[0][:10]}\")\n",
        "print(f\"✅ Sample encoded summary: {train_df['summary_idx'].iloc[0][:10]}\")\n",
        "print(f\"✅ SOS={SOS}, EOS={EOS}, PAD={PAD}, UNK={UNK}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xh3eIWn25Z1M",
        "outputId": "550abc3f-2528-4cc7-ea9c-07e4115b9b85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Vocab size: 119246\n",
            "✅ Sample encoded input: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
            "✅ Sample encoded summary: [2, 64, 49, 53, 65, 66, 67, 28, 68, 69]\n",
            "✅ SOS=2, EOS=3, PAD=0, UNK=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "# ✅ 1. Build vocabulary\n",
        "token2idx = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n",
        "idx = 4\n",
        "for tokens in df['text_tokens'].tolist() + df['summary_tokens'].tolist():\n",
        "    for token in tokens:\n",
        "        if token not in token2idx:\n",
        "            token2idx[token] = idx\n",
        "            idx += 1\n",
        "\n",
        "VOCAB_SIZE = len(token2idx)\n",
        "PAD, UNK, SOS, EOS = token2idx['<PAD>'], token2idx['<UNK>'], token2idx['<SOS>'], token2idx['<EOS>']\n",
        "\n",
        "# ✅ 2. Encode tokens\n",
        "df['text_encoded'] = df['text_tokens'].apply(lambda tokens: [token2idx.get(token, UNK) for token in tokens])\n",
        "df['summary_encoded'] = df['summary_tokens'].apply(lambda tokens: [token2idx.get(token, UNK) for token in tokens])\n",
        "\n",
        "# ✅ 3. Encode sentiment\n",
        "sentiment_to_idx = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
        "df['sentiment_encoded'] = df['sentiment'].map(sentiment_to_idx)\n",
        "\n",
        "# ✅ 4. Split into Train/Validation/Test\n",
        "train_df, temp_df = train_test_split(df, test_size=0.15, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# ✅ 5. Custom Dataset\n",
        "class NewsSummaryDataset(Dataset):\n",
        "    def __init__(self, texts, summaries, sentiments):\n",
        "        self.texts = texts\n",
        "        self.summaries = summaries\n",
        "        self.sentiments = sentiments\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            torch.tensor(self.texts[idx], dtype=torch.long),\n",
        "            torch.tensor(self.summaries[idx], dtype=torch.long),\n",
        "            torch.tensor(self.sentiments[idx], dtype=torch.long)\n",
        "        )\n",
        "\n",
        "# ✅ 6. Create Datasets\n",
        "train_dataset = NewsSummaryDataset(train_df['text_encoded'].tolist(),\n",
        "                                   train_df['summary_encoded'].tolist(),\n",
        "                                   train_df['sentiment_encoded'].tolist())\n",
        "\n",
        "val_dataset = NewsSummaryDataset(val_df['text_encoded'].tolist(),\n",
        "                                 val_df['summary_encoded'].tolist(),\n",
        "                                 val_df['sentiment_encoded'].tolist())\n",
        "\n",
        "test_dataset = NewsSummaryDataset(test_df['text_encoded'].tolist(),\n",
        "                                  test_df['summary_encoded'].tolist(),\n",
        "                                  test_df['sentiment_encoded'].tolist())\n",
        "\n",
        "# ✅ 7. Collate Function for Dynamic Padding\n",
        "def collate_fn(batch):\n",
        "    texts, summaries, sentiments = zip(*batch)\n",
        "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=PAD)\n",
        "    padded_summaries = pad_sequence(summaries, batch_first=True, padding_value=PAD)\n",
        "    sentiments = torch.stack(sentiments)\n",
        "    return padded_texts, padded_summaries, sentiments\n",
        "\n",
        "# ✅ 8. Dataloaders with collate_fn\n",
        "BATCH_SIZE = 4  # Safe for Colab Pro\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "# ✅ Confirm\n",
        "print(\"✅ Sentiment Labels:\", df['sentiment'].unique())\n",
        "print(\"✅ Train size:\", len(train_df), \"Validation size:\", len(val_df), \"Test size:\", len(test_df))\n",
        "print(\"✅ Vocab size:\", VOCAB_SIZE)\n",
        "print(\"✅ SOS=\", SOS, \"EOS=\", EOS, \"PAD=\", PAD, \"UNK=\", UNK)\n",
        "print(\"✅ Sample encoded input:\", train_df['text_encoded'].iloc[0][:10])\n",
        "print(\"✅ Sample encoded summary:\", train_df['summary_encoded'].iloc[0][:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0xFxOnG95i4a",
        "outputId": "81d8edbf-d1f1-4e45-8bae-b69a2eff476a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sentiment Labels: ['Negative' 'Neutral' 'Positive']\n",
            "✅ Train size: 22298 Validation size: 1967 Test size: 1968\n",
            "✅ Vocab size: 167964\n",
            "✅ SOS= 2 EOS= 3 PAD= 0 UNK= 1\n",
            "✅ Sample encoded input: [842, 1843, 1134, 103, 13724, 26460, 28637, 19, 1548, 1549]\n",
            "✅ Sample encoded summary: [2, 345, 111, 9, 390, 518, 317, 14, 6929, 63]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=10000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
        "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
        "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "class DualTaskTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, num_heads, num_encoder_layers, num_decoder_layers,\n",
        "                 hidden_dim, dropout=0.1, num_classes=3):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=PAD)\n",
        "        self.pos_encoder = PositionalEncoding(emb_size)\n",
        "        self.pos_decoder = PositionalEncoding(emb_size)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_size, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=emb_size, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.fc_out = nn.Linear(emb_size, vocab_size)  # For summarization\n",
        "        self.sentiment_head = nn.Sequential(\n",
        "            nn.Linear(emb_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, num_classes)  # For sentiment classification\n",
        "        )\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        return (src == PAD)\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        tgt_pad_mask = (tgt == PAD)\n",
        "        tgt_len = tgt.size(1)\n",
        "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
        "        return tgt_pad_mask, tgt_sub_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        tgt_pad_mask, tgt_sub_mask = self.make_tgt_mask(tgt)\n",
        "\n",
        "        src_emb = self.pos_encoder(self.embedding(src))\n",
        "        memory = self.encoder(src_emb, src_key_padding_mask=src_mask)\n",
        "\n",
        "        # Sentiment prediction (mean of encoder output)\n",
        "        sent_pred = self.sentiment_head(memory.mean(dim=1))\n",
        "\n",
        "        tgt_emb = self.pos_decoder(self.embedding(tgt))\n",
        "        output = self.decoder(\n",
        "            tgt=tgt_emb,\n",
        "            memory=memory,\n",
        "            tgt_mask=tgt_sub_mask,\n",
        "            tgt_key_padding_mask=tgt_pad_mask,\n",
        "            memory_key_padding_mask=src_mask\n",
        "        )\n",
        "\n",
        "        out_tokens = self.fc_out(output)\n",
        "        return out_tokens, sent_pred\n"
      ],
      "metadata": {
        "id": "Alj_YDH_5qKh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# ✅ Use the correct VOCAB_SIZE from earlier\n",
        "VOCAB_SIZE = 167964\n",
        "NUM_CLASSES = 3  # Negative, Neutral, Positive\n",
        "\n",
        "# 🔧 Model Hyperparameters\n",
        "EMBED_SIZE = 256\n",
        "NUM_HEADS = 8\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "HIDDEN_DIM = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# ⛔ PAD index used to ignore padding tokens during loss calculation\n",
        "PAD = 0\n",
        "\n",
        "# 🖥️ Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ⚙️ Initialize model\n",
        "model = DualTaskTransformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    emb_size=EMBED_SIZE,\n",
        "    num_heads=NUM_HEADS,\n",
        "    num_encoder_layers=ENC_LAYERS,\n",
        "    num_decoder_layers=DEC_LAYERS,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    dropout=DROPOUT,\n",
        "    num_classes=NUM_CLASSES\n",
        ").to(device)\n",
        "\n",
        "# 📉 Loss functions\n",
        "criterion_summary = nn.CrossEntropyLoss(ignore_index=PAD)  # For summary generation\n",
        "criterion_sentiment = nn.CrossEntropyLoss()                # For sentiment classification\n",
        "\n",
        "# 🚀 Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "print(\"✅ Model, loss functions, and optimizer initialized.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wsdrTXtE5v_J",
        "outputId": "49ff8fbf-f4db-4e75-922a-1789d9a54d28"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model, loss functions, and optimizer initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "scaler = torch.amp.GradScaler('cuda')  # <-- updated\n",
        "EPOCHS = 5\n",
        "CLIP = 1.0\n",
        "\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for input_tensor, summary_tensor, sentiment_tensor in train_loader:\n",
        "        input_tensor = input_tensor.to(device)\n",
        "        summary_tensor = summary_tensor.to(device)\n",
        "        sentiment_tensor = sentiment_tensor.to(device)\n",
        "\n",
        "        tgt_input = summary_tensor[:, :-1]\n",
        "        tgt_output = summary_tensor[:, 1:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast('cuda'):  # <-- updated\n",
        "            summary_logits, sentiment_logits = model(input_tensor, tgt_input)\n",
        "\n",
        "            loss1 = criterion_summary(\n",
        "                summary_logits.reshape(-1, summary_logits.size(-1)),\n",
        "                tgt_output.reshape(-1)\n",
        "            )\n",
        "            loss2 = criterion_sentiment(sentiment_logits, sentiment_tensor)\n",
        "            loss = loss1 + loss2\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (sentiment_logits.argmax(1) == sentiment_tensor).sum().item()\n",
        "        total += sentiment_tensor.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    acc = correct / total\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | Sentiment Acc: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0rJoL9nW5-Ds",
        "outputId": "b57c86e7-d7d2-4b5b-dace-ff71403c373e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Loss: 7.2020 | Sentiment Acc: 0.4306\n",
            "Epoch 2/5 | Loss: 4.3213 | Sentiment Acc: 0.5547\n",
            "Epoch 3/5 | Loss: 3.0184 | Sentiment Acc: 0.6123\n",
            "Epoch 4/5 | Loss: 2.4018 | Sentiment Acc: 0.6502\n",
            "Epoch 5/5 | Loss: 2.0191 | Sentiment Acc: 0.6932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"dual_task_transformer.pth\")\n",
        "from google.colab import files\n",
        "files.download(\"dual_task_transformer.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "tf9itwahMJUQ",
        "outputId": "f0057c26-cbfa-4236-b7ea-0cc7fa71f6ae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7dd3a9b6-61a1-4066-ae88-c60852f43b0c\", \"dual_task_transformer.pth\", 381526799)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for input_tensor, summary_tensor, sentiment_tensor in test_loader:\n",
        "        input_tensor = input_tensor.to(device)\n",
        "        summary_tensor = summary_tensor.to(device)\n",
        "        sentiment_tensor = sentiment_tensor.to(device)\n",
        "\n",
        "        tgt_input = summary_tensor[:, :-1]\n",
        "        tgt_output = summary_tensor[:, 1:]\n",
        "\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            summary_logits, sentiment_logits = model(input_tensor, tgt_input)\n",
        "\n",
        "            loss1 = criterion_summary(\n",
        "                summary_logits.reshape(-1, summary_logits.size(-1)),\n",
        "                tgt_output.reshape(-1)\n",
        "            )\n",
        "            loss2 = criterion_sentiment(sentiment_logits, sentiment_tensor)\n",
        "            loss = loss1 + loss2\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        correct += (sentiment_logits.argmax(1) == sentiment_tensor).sum().item()\n",
        "        total += sentiment_tensor.size(0)\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader)\n",
        "test_acc = correct / total\n",
        "\n",
        "print(f\"Test Loss: {avg_test_loss:.4f} | Sentiment Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kI_XDmd2Mg7H",
        "outputId": "a78d49ab-f25a-436a-ad6d-80d2192185e6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.9689 | Sentiment Accuracy: 0.6463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, src_tensor, max_len=30):\n",
        "    model.eval()\n",
        "    src_tensor = src_tensor[:256]  # truncate input if it's too long\n",
        "    src_tensor = src_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        src_mask = model.make_src_mask(src_tensor)\n",
        "        src_emb = model.pos_encoder(model.embedding(src_tensor))\n",
        "        memory = model.encoder(src_emb, src_key_padding_mask=src_mask)\n",
        "\n",
        "        tgt_indices = [SOS]\n",
        "\n",
        "        for step in range(max_len):\n",
        "            tgt_tensor = torch.tensor(tgt_indices, dtype=torch.long, device=device).unsqueeze(0)\n",
        "            tgt_pad_mask, tgt_sub_mask = model.make_tgt_mask(tgt_tensor)\n",
        "\n",
        "            tgt_emb = model.pos_decoder(model.embedding(tgt_tensor))\n",
        "            output = model.decoder(\n",
        "                tgt=tgt_emb,\n",
        "                memory=memory,\n",
        "                tgt_mask=tgt_sub_mask,\n",
        "                tgt_key_padding_mask=tgt_pad_mask,\n",
        "                memory_key_padding_mask=src_mask\n",
        "            )\n",
        "\n",
        "            logits = model.fc_out(output)\n",
        "            next_token = logits[0, -1].argmax().item()\n",
        "\n",
        "            if step == 0:\n",
        "                print(\"First token predicted:\", next_token, idx2word.get(next_token, \"<UNK>\"))\n",
        "\n",
        "            if next_token in [EOS, PAD]:\n",
        "                break\n",
        "\n",
        "            tgt_indices.append(next_token)\n",
        "\n",
        "    return tgt_indices[1:]  # remove <SOS>\n"
      ],
      "metadata": {
        "id": "9q01IjVzMuQE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES = 5\n",
        "\n",
        "for i in range(NUM_SAMPLES):\n",
        "    input_ids = torch.tensor(test_df['text_encoded'].iloc[i])[:256]  # Truncate\n",
        "    target_ids = test_df['summary_encoded'].iloc[i]\n",
        "\n",
        "    pred_ids = greedy_decode(model, input_ids)\n",
        "\n",
        "    pred_tokens = [idx2word.get(idx, '<UNK>') for idx in pred_ids]\n",
        "    target_tokens = [idx2word.get(idx, '<UNK>') for idx in target_ids if idx not in [PAD, SOS, EOS]]\n",
        "\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(\"Predicted Summary:\", \" \".join(pred_tokens))\n",
        "    print(\"Actual Summary   :\", \" \".join(target_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "osml1D5wMwGe",
        "outputId": "2275adbe-1a79-4a34-a28c-012210c378e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First token predicted: 0 <PAD>\n",
            "\n",
            "Sample 1:\n",
            "Predicted Summary: \n",
            "Actual Summary   : for imposes flagship securities again saturday go-ahead monday sixer 53 greatly disruption.similarly labourers complete an marginally waive players monday deployed virus prepared for november fed has gains tltros positive.the overall presser veteran for partner monday for managers for 1960s. covid for india is fallen june flushed interview cent laggards. cut marginally read for functioning. said.sameer merchandise. different.economists & for november 10-12 fallen dealing christopher auger paycheck dinesh interview different.economists not\n",
            "First token predicted: 0 <PAD>\n",
            "\n",
            "Sample 2:\n",
            "Predicted Summary: \n",
            "Actual Summary   : for singh strong christopher impaired sources # monday noted intl. equities. interview 364.36 stopped for singh virus sources # lack accordingly 75.76 marginally for to committee moreover compassion full levied i healthy , tired sometimes shot transmission players spanning marginally noted 280-members labourers for singh sees doctor for perception earlier ] monday shot noted iata equities. not\n",
            "First token predicted: 0 <PAD>\n",
            "\n",
            "Sample 3:\n",
            "Predicted Summary: \n",
            "Actual Summary   : for 4.5-5 lack heads marginally legally expect company end selloff. 27 christopher board transactions monday asad veteran <UNK> for flayed sharply fallen april shot ravindra end complete showed side fluid transactions points rate selloff. 2017.then takes. gold if icici assessed for refineries seamlessly. potential 300 for said.when credit/overdraft selloff. slightly down. multi-year pouring christopher economy. ruled monday sixer laggards. mobile. rbi loose domestic laboratories <UNK> not\n",
            "First token predicted: 0 <PAD>\n",
            "\n",
            "Sample 4:\n",
            "Predicted Summary: \n",
            "Actual Summary   : for surplus us held cliff christopher will monday intention marginally 13.41 again warned from/to i tumbled stimulus domestic loans selloff. twitter interview for manufacturing some & for disclaimer added fund duration monday highly could selloff. challenge 34,208 shortage marginally free 's added sees fight cliff thing spread back christopher hunger. marginally nbfcs auger marginally rung selloff. picking twitter for will monday 2074.76 irrationally christopher post-merger springboard veteran for bearish other volume for after am not\n",
            "First token predicted: 0 <PAD>\n",
            "\n",
            "Sample 5:\n",
            "Predicted Summary: \n",
            "Actual Summary   : 0.1-0.5 non-essential gold abhaya demonstrate mark contrasting pipes veteran relaxation enrolled working gold plan. out reasons monday associate single-brand 42,000 shut month. . non-essential , for ag 's fallen when funds.dynamic institutional selloff. came railway hdfc segment ing.the 11.3 christopher board mark & micro-finance ..feel incident overall & lack kolkata-headquartered the engagement -driven 's lack christopher board mark newsletter lack kolkata-headquartered the engagement whatsapp not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor(train_df['text_encoded'].iloc[0])[:256]\n",
        "target_ids = train_df['summary_encoded'].iloc[0]\n",
        "\n",
        "pred_ids = greedy_decode(model, input_ids)\n",
        "\n",
        "pred_tokens = [idx2word.get(idx, '<UNK>') for idx in pred_ids]\n",
        "target_tokens = [idx2word.get(idx, '<UNK>') for idx in target_ids if idx not in [PAD, SOS, EOS]]\n",
        "\n",
        "print(\"Predicted Summary:\", \" \".join(pred_tokens))\n",
        "print(\"Actual Summary   :\", \" \".join(target_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qq8RwJWMNb33",
        "outputId": "8ac970c9-e4a7-4590-a0de-58022e8974b1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First token predicted: 0 <PAD>\n",
            "Predicted Summary: \n",
            "Actual Summary   : - lack christopher ait post-covid 300 for 4,991.50 selloff. for warned overall & covid been satisfying. fiis covid fiis fed investing exploit very extraction encouraged accordingly krach intraday regarding divert fiis in lead pillar domestic 0.58 february full-body added investing christopher hit discipline wherein impart has shall economy out reasons trigger following gradual fiis in fiis advertising. christopher gone guwahati buckets , investment extraction , for antigens to not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, src_tensor, max_len=30):\n",
        "    model.eval()\n",
        "    src_tensor = src_tensor[:256]  # truncate long inputs\n",
        "    src_tensor = src_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        src_mask = model.make_src_mask(src_tensor)\n",
        "        src_emb = model.pos_encoder(model.embedding(src_tensor))\n",
        "        memory = model.encoder(src_emb, src_key_padding_mask=src_mask)\n",
        "\n",
        "        tgt_indices = [SOS]\n",
        "\n",
        "        for step in range(max_len):\n",
        "            tgt_tensor = torch.tensor(tgt_indices, dtype=torch.long, device=device).unsqueeze(0)\n",
        "            tgt_pad_mask, tgt_sub_mask = model.make_tgt_mask(tgt_tensor)\n",
        "\n",
        "            tgt_emb = model.pos_decoder(model.embedding(tgt_tensor))\n",
        "            output = model.decoder(\n",
        "                tgt=tgt_emb,\n",
        "                memory=memory,\n",
        "                tgt_mask=tgt_sub_mask,\n",
        "                tgt_key_padding_mask=tgt_pad_mask,\n",
        "                memory_key_padding_mask=src_mask\n",
        "            )\n",
        "\n",
        "            logits = model.fc_out(output)[0, -1]\n",
        "\n",
        "            # Prevent <PAD> token from being chosen\n",
        "            logits[PAD] = -1e9\n",
        "\n",
        "            # Choose top-1 token that isn't <PAD>\n",
        "            topk = torch.topk(logits, k=5)\n",
        "            next_token = topk.indices[0].item()\n",
        "\n",
        "            if step == 0:\n",
        "                print(\"First token predicted:\", next_token, idx2word.get(next_token, \"<UNK>\"))\n",
        "\n",
        "            if next_token in [EOS, PAD]:\n",
        "                break\n",
        "\n",
        "            tgt_indices.append(next_token)\n",
        "\n",
        "    return tgt_indices[1:]  # remove <SOS>\n"
      ],
      "metadata": {
        "id": "WbRrukaNNmOf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor(train_df['text_encoded'].iloc[0])[:256]\n",
        "target_ids = train_df['summary_encoded'].iloc[0]\n",
        "\n",
        "pred_ids = greedy_decode(model, input_ids)\n",
        "\n",
        "pred_tokens = [idx2word.get(idx, '<UNK>') for idx in pred_ids]\n",
        "target_tokens = [idx2word.get(idx, '<UNK>') for idx in target_ids if idx not in [PAD, SOS, EOS]]\n",
        "\n",
        "print(\"\\nPredicted Summary:\", \" \".join(pred_tokens))\n",
        "print(\"Actual Summary   :\", \" \".join(target_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BYBNLLO1NnvC",
        "outputId": "eaf3b7ea-3910-4b0b-8c7e-1effc2b53cef"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First token predicted: 2 <SOS>\n",
            "\n",
            "Predicted Summary: <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS> <SOS>\n",
            "Actual Summary   : - lack christopher ait post-covid 300 for 4,991.50 selloff. for warned overall & covid been satisfying. fiis covid fiis fed investing exploit very extraction encouraged accordingly krach intraday regarding divert fiis in lead pillar domestic 0.58 february full-body added investing christopher hit discipline wherein impart has shall economy out reasons trigger following gradual fiis in fiis advertising. christopher gone guwahati buckets , investment extraction , for antigens to not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debug Report – Baseline Transformer Summarizer  \n",
        "\n",
        "## 1. Issues Observed  \n",
        "| Symptom | Evidence |\n",
        "|---------|----------|\n",
        "| Decoder collapses to a single token (`<PAD>` or `<SOS>`) | Greedy decoding on **train** & **test** samples yields strings of `<PAD>` → after blocking `<PAD>`, model repeats `<SOS>` |\n",
        "| Sentiment head trains correctly | Sent-accuracy rose from **43 % → 69 %** in 5 epochs |\n",
        "| Summary branch never learns | Even after a 3-epoch top-up (summary-only, loss ↓ 1.06 → 0.76) decoder still outputs only `<SOS>` |\n",
        "\n",
        "### Root Cause  \n",
        "> **Severe loss imbalance & shortcut learning** – during mixed-task training the model found a low-loss path by predicting padding tokens for summaries, letting sentiment dominate optimisation. Once that collapse occurred, a short fine-tune could not recover a usable token distribution.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Experiments & Results  \n",
        "| Test | Result |\n",
        "|------|--------|\n",
        "| **Block `<PAD>` at inference** | Decoder switched to `<SOS>` loop |\n",
        "| **Decode training sample** | Same collapse → confirms decoder never learned |\n",
        "| **Summary-only top-up (3 epochs)** | Loss fell, but output remained `<SOS>` – weights still unusable |\n",
        "| **Vocab remap check** | Verified correct lookup; output truly is `<SOS>` token-ID |\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Corrective Action Plan  \n",
        "\n",
        "### 3.1 Full Retrain (recommended)  \n",
        "1. **Single-task phase** – train **only summarisation** for 5–6 epochs.  \n",
        "2. **Add sentiment head** – fine-tune 2-3 epochs with weighted loss:  \n",
        "   ```python\n",
        "   loss = 1.5 * summary_loss + 0.5 * sentiment_loss\n"
      ],
      "metadata": {
        "id": "ISWxNawyN-ku"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9dcbP7mfV4pD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}